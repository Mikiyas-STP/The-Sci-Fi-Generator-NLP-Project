{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5169490e",
   "metadata": {},
   "source": [
    "# Sci-Fi Story Generator using RNN (LSTM)\n",
    "## Final Project - NLP for Creatives (Peckham DAZ Programme 2024/25)\n",
    "### üîç Project Overview\n",
    "\n",
    "This project explores how Recurrent Neural Networks (RNNs), specifically Long Short-Term Memory (LSTM) networks, can be used to generate sci-fi stories. A dataset of public domain science fiction texts is used to train a language model that learns how to predict the next word in a sequence and generate creative story snippets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534cb4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.12/site-packages/huggingface_hub-0.29.2-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82dd8ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load from Project Gutenberg (sci-fi book)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.gutenberg.org/cache/epub/2147/pg2147.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Basic cleaning\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "# Load from Project Gutenberg (sci-fi book)\n",
    "url = \"http://www.gutenberg.org/cache/epub/2147/pg2147.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Basic cleaning\n",
    "text = text.lower()\n",
    "text = re.sub(r'[^a-z\\s]', '', text)\n",
    "nltk.download('punkt')\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"First 50 tokens:\", tokens[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34976dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokens)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create sequences\n",
    "input_sequences = []\n",
    "seq_length = 50\n",
    "\n",
    "for i in range(seq_length, len(tokens)):\n",
    "    seq = tokens[i-seq_length:i+1]\n",
    "    encoded = tokenizer.texts_to_sequences([seq])[0]\n",
    "    input_sequences.append(encoded)\n",
    "\n",
    "# Pad and split into X and y\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length+1))\n",
    "X = input_sequences[:, :-1]\n",
    "y = to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
    "\n",
    "print(\"Vocabulary size:\", total_words)\n",
    "print(\"Input shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271726dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "‚ö†Ô∏è Note: Training might take a while, especially without GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words=50):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted)\n",
    "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"in the year 3000 a group of astronauts discovered\"\n",
    "story = generate_text(seed, next_words=50)\n",
    "print(\"‚ú® Generated Sci-Fi Text:\\n\", story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db00a1",
   "metadata": {},
   "source": [
    "### üß† Reflections and Ethical Notes\n",
    "\n",
    "- **Learning experience:** I learned how to build and train a language model using RNNs.\n",
    "- **Challenges:** Memory usage and training time were the main challenges. Using Google Colab helped.\n",
    "- **Limitations:** The model sometimes repeats phrases or generates grammatically incorrect text.\n",
    "- **Ethics:** I used public domain data. The model is intended for creative storytelling, not factual information.\n",
    "- **LLM Disclaimer:** I used ChatGPT to assist in structuring this project, coding, and documentation. All code was tested and understood by me.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
